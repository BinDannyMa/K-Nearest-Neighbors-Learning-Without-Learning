{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN: Curse of Dimensionality\n",
    "\n",
    "\n",
    "\n",
    "In this notebook we perform multiclass classification using K Nearest Neighbors (K-NN) model on a **high-dimensional large dataset**. Our goal is to understand the problem of the curse of dimensionality that the K-NN model suffers from.  \n",
    "\n",
    "The curse of dimensionality affects the K-NN model in four ways.\n",
    "- In high-dimension all neighbors are far away (outliers)!\n",
    "- Distance calculation is expensive in high-dimension.\n",
    "- Number of required training data increases exponentially.\n",
    "- Large number of irrelevant features in high-dimensional data.\n",
    "\n",
    "In many practical problems the 1st problem doesn't arise as it is based on the assumption that the data is uniformly distributed (it doesn't hold true). Moreover, the data may have thousands of features, but in reality they all \"live\" in a much lower-dimensional space. For example, in handwritten digit recognition problem, each pixel is a dimension. \n",
    "\n",
    "In this notebook we investigate the 2nd problem of the curse of dimensionality:\n",
    "        \n",
    "        Distance calculation is expensive in high-dimension.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "We perform the following three tasks:\n",
    "\n",
    "- Task 1: We use a high-dimensional and large dataset to show that distance calculation is expensive (time-consuming), that results into **awfully large \"training\" time** using a K-NN model.\n",
    "\n",
    "- Task 2: We compare the \"training time\" of K-NN model (48 minues) with that of the Random Forest model (2+ minutes), and show that it takes about 20 times less time to train a Random Forest classifier on the same dataset.\n",
    "\n",
    "- Task 3: Finally, to combat the curse of dimensionality in K-NN, we apply the dimensionality reduction technique and show that the K-NN \"training\" time can be significantly reduced (2 min 40 sec). \n",
    "\n",
    "\n",
    "## Dataset: MNIST\n",
    "\n",
    "\n",
    "We use the MNIST (Modified National Institute of Standards and Technology) dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "\n",
    "There are 70,000 images. Each image is 28x28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black).\n",
    "\n",
    "Thus, each image has 784 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Create Data Matrix (X) & Target Vector (y)\n",
    "\n",
    "First load the data and explore the feature names, target names, etc.\n",
    "\n",
    "We may load the data from a local folder or load it directly from cloud using Scikit-Learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of Samples:  (70000, 784)\n",
      "No. of Labels:  (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the local folder \"data\"\n",
    "mnist = loadmat('data/mnist-original.mat')\n",
    "\n",
    "# Create the data Matrix X and the target vector y\n",
    "X = mnist[\"data\"].T.astype('float32')\n",
    "y = mnist[\"label\"][0].astype('int64')\n",
    "\n",
    "\n",
    "# Load data using Scikit-Learn\n",
    "# mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "# X = mnist[\"data\"].astype('float32')\n",
    "# y = mnist[\"target\"].astype('int64')\n",
    "\n",
    "\n",
    "print(\"\\nNo. of Samples: \", X.shape)\n",
    "print(\"No. of Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a Random Image\n",
    "\n",
    "To display a digit, we need to reshape an instance’s feature vector to a 28 x 28 array. \n",
    "\n",
    "For displaying we use Matplotlib’s imshow() function:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_digit = X[33538]\n",
    "\n",
    "random_digit_image = random_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(random_digit_image, cmap = plt.cm.binary, interpolation=\"nearest\")\n",
    "#plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "Each image of the MNIST dataset is encoded in a 784 dimensional vector, representing a 28 x 28 pixel image. Each pixel has a value between 0 and 255, corresponding to the grey-value of a pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Saling\n",
    "\n",
    "To avoid big weights that deal with the pixel values from between [0, 255], we scale X down. \n",
    "\n",
    "A commonly used range is [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X /= 255.0\n",
    "\n",
    "X.min(), X.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Dataset\n",
    "\n",
    "We use sklearn's train_test_split function to spilt the dataset into training and test subsets. The data is shuffled by default before splitting.\n",
    "\n",
    "This function splits arrays or matrices into **random** train and test subsets.\n",
    "\n",
    "For the **reproducibility of the results**, we need to use the same seed for the random number generator. The seed is set by the \"random_state\" parameter of the split function. \n",
    "\n",
    "However, in repeated experiments if we don't want to use the same train and test subsets, then we drop the \"random_state\" parameter from the funtion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Demonstration of Large \"Training\" Time using a K-NN Model \n",
    "\n",
    "\n",
    "\n",
    "We use Scikit-Learn's KNeighborsClassifier for this demonstration.\n",
    "\n",
    "## KNeighborsClassifier Parameters\n",
    "\n",
    "We need to set the following parameters to train a KNeighborsClassifier.\n",
    "\n",
    "\n",
    "- n_neighbors : int, optional (default = 5) Number of neighbors to use by default for kneighbors queries.\n",
    "\n",
    "- weights : str or callable, optional (default = ‘uniform’) weight function used in prediction. Possible values:\n",
    "\n",
    "    -- ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "    \n",
    "    -- ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "    \n",
    "    -- [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "- algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional. Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "    -- ‘ball_tree’ will use BallTree\n",
    "    \n",
    "    -- ‘kd_tree’ will use KDTree\n",
    "    \n",
    "    -- ‘brute’ will use a brute-force search.\n",
    "    \n",
    "    -- ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "    \n",
    "    -- Note: fitting on sparse input will override the setting of this parameter, using brute force.\n",
    "\n",
    "- leaf_size : int, optional (default = 30). Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "\n",
    "- p : integer, optional (default = 2)\n",
    "    \n",
    "    -- Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "\n",
    "- metric : string or callable, default ‘minkowski’. the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics.\n",
    "\n",
    "- metric_params : dict, optional (default = None). Additional keyword arguments for the metric function.\n",
    "\n",
    "- n_jobs : int or None, optional (default=None). The number of parallel jobs to run for neighbors search. \n",
    "\n",
    "    -- n_jobs= None means 1 unless in a joblib.parallel_backend context. \n",
    "   \n",
    "    -- n_jobs = -1 means using all processors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Choose a Combination of Optimal Parameters\n",
    "\n",
    "A K-NN model is defined by a set of parameters: nearest neighbors (K), distance metric (p), etc. These are called **hyperparameters**. We need to select the best model based on the optimal values of these hyperparameters. This process is called hyperparameter tuning.\n",
    "\n",
    "The model parameters are known as **hyperparameters** because the values of these parameters are used to control the learning process. By contrast, the values of other parameters are learned. In the K-NN model there is no learning, thus we only tune the hyperparameters to create the optimal model\n",
    "\n",
    "Hyperparameters are are passed as arguments to the constructor of the estimator classes. We search the hyperparameter space for the best cross validation score.\n",
    "\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "The best way to do hyperparameter tuning is to use cross-validation.\n",
    "\n",
    "We will use Scikit-Learn’s **GridSearchCV** to search the combinations of hyperparameter values that provide the best performance.\n",
    "\n",
    "We need to tell which hyperparameters we want the GridSearchCV to experiment with, and what values to try out. It will evaluate all the possible combinations of hyperparameter values, using cross-validation. \n",
    "\n",
    "We will tune the following hyperparameters of the GridSearchCV model.\n",
    "\n",
    "- n_neighbors \n",
    "- weights \n",
    "- p \n",
    "    \n",
    "\n",
    "\n",
    "### GridSearchCV Parameters\n",
    "\n",
    "Following are the most frequenly used parameters by GridSearchCV.\n",
    "\n",
    "- estimator : estimator object. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed.\n",
    "\n",
    "- param_grid : Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.\n",
    "\n",
    "- scoring : string, callable, list/tuple, dict or None, default: None\n",
    "\n",
    "    -- A single string (see The scoring parameter: defining model evaluation rules) or a callable (see Defining your scoring strategy from metric functions) to evaluate the predictions on the test set.\n",
    "\n",
    "\n",
    "- n_jobs : int or None, optional (default=None). Number of jobs to run in parallel. \n",
    "\n",
    "    -- n_jobs= None means 1 unless in a joblib.parallel_backend context. \n",
    "   \n",
    "    -- n_jobs = -1 means using all processors. \n",
    "\n",
    "- cv : int. Determines the cross-validation splitting strategy. None, to use the default 3-fold cross validation.\n",
    "\n",
    "- verbose : integer. Controls the verbosity: the higher, the more messages.\n",
    "\n",
    "\n",
    "### Note: the scoring function\n",
    "\n",
    "The GridSearchCV takes an argument to define the scoring metric (performance measure). \n",
    "\n",
    "See the list of possible scoring functions:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "For multiclass classification, we may use \"f1_micro\" scoring function. The f1_micro function is the average of the F1 score of each class with weighting depending on the average parameter.\n",
    "\n",
    "In the binary classification, f1 score function can be used. We may also use the precision, recall, roc_auc functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 257.2min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 1124.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.972143\n",
      "Optimal Hyperparameter Values:  {'n_neighbors': 4, 'p': 2, 'weights': 'distance'}\n",
      "\n",
      "\n",
      "Wall time: 18h 45min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The param_grid tells Scikit-Learn to evaluate all combinations of the hyperparameter values\n",
    "param_grid = {'n_neighbors': [3, 4, 5, 9], 'p': [1, 2, 1000], 'weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(knn_clf, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "params_optimal_knn = knn_cv.best_params_\n",
    "\n",
    "print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_knn)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation: Time Required for Hyperparameter Tunining \n",
    "\n",
    "We observe that it takes about **19 hours** to perform hyperparameter tunining with only a few values for 3 hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select The Best Model \n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy:  1.0\n",
      "Wall time: 47min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knn = KNeighborsClassifier(**params_optimal_knn)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_train_predicted = knn.predict(X_train)\n",
    "\n",
    "train_accuracy_knn = np.mean(y_train_predicted == y_train)\n",
    "print(\"\\nTraining Accuracy: \", train_accuracy_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Observation: Time Required for \"Training\" a K-NN Model \n",
    "\n",
    "We observe that it takes about **48 minutes** to train a \"K-NN\" classifier.\n",
    "\n",
    "Later, in task 2, we will see that using Random Forest classifier it takes **less than 3 minutes** to train on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalute The Model Using Confusion Matrix for Training Data\n",
    "\n",
    "\n",
    "We use the cross_val_predict() function to performs cross-validation, \n",
    "\n",
    "However, unlike cross_val_score, it doesn't return the evaluation scores.\n",
    "\n",
    "Instead it returns the predictions made on each test fold. \n",
    "\n",
    "This means that we get a clean prediction for each instance in the training set.\n",
    "\n",
    "By “clean” we mean that the prediction is made by a model that never saw the data during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5549,    6,    5,    0,    0,    8,   14,    2,    1,    6],\n",
       "       [   1, 6239,    8,    1,    5,    1,    3,   10,    2,    3],\n",
       "       [  34,   44, 5433,   20,    5,    3,    5,   78,   16,    4],\n",
       "       [   5,   12,   30, 5509,    1,   61,    2,   38,   36,   20],\n",
       "       [   2,   38,    2,    1, 5290,    1,   11,    8,    2,  107],\n",
       "       [  12,    5,    3,   54,    8, 4868,   47,    6,    3,   27],\n",
       "       [  25,   14,    0,    0,    8,   20, 5408,    0,    4,    0],\n",
       "       [   0,   61,   11,    0,    9,    1,    0, 5690,    5,   55],\n",
       "       [  16,   52,   14,   71,   23,   72,   21,   17, 5092,   57],\n",
       "       [  12,   12,    4,   26,   36,   12,    3,   66,    6, 5362]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_pred = cross_val_predict(knn, X_train, y_train, cv=5)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Model Using Test Data\n",
    "\n",
    "The KNeighborsClassifier model has a default \"score\" function that computes the accuracy of the model.\n",
    "\n",
    "Often times the accuracy is not a good measure.\n",
    "\n",
    "We also create the confusion matrix for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9744285714285714\n",
      "\n",
      "No. of correct predictions (Test): 13642/14000\n",
      "\n",
      "Confusion Matrix (Test Data):\n",
      " [[1305    0    0    0    0    0    6    1    0    0]\n",
      " [   0 1595    2    0    0    0    1    4    0    2]\n",
      " [   7    8 1311    4    0    0    1   15    0    2]\n",
      " [   0    4    9 1377    1   18    0    6    8    4]\n",
      " [   1   11    0    0 1311    0    7    3    1   28]\n",
      " [   2    0    2   14    0 1238   13    1    5    5]\n",
      " [   3    1    1    0    2    3 1386    0    1    0]\n",
      " [   1    9    5    1    4    0    0 1428    1   12]\n",
      " [   1   14    1   15    5   14    6    1 1324    9]\n",
      " [   4    3    2    9   18    2    1   10    3 1367]]\n",
      "Wall time: 23min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# The accuracy of the model\n",
    "test_accuracy_knn = knn.score(X_test, y_test)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy_knn)\n",
    "\n",
    "\n",
    "# No. of Correct Predictions\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "print(\"\\nNo. of correct predictions (Test): %d/%d\" % (np.sum(y_test_predicted == y_test), len(y_test)))\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Test Data\n",
    "\n",
    "Using pandas crosstab, we create a better visualization of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1595</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1311</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1377</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1311</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1238</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1386</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1428</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1324</td>\n",
       "      <td>9</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1367</td>\n",
       "      <td>1419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>All</td>\n",
       "      <td>1324</td>\n",
       "      <td>1645</td>\n",
       "      <td>1333</td>\n",
       "      <td>1420</td>\n",
       "      <td>1341</td>\n",
       "      <td>1275</td>\n",
       "      <td>1421</td>\n",
       "      <td>1469</td>\n",
       "      <td>1343</td>\n",
       "      <td>1429</td>\n",
       "      <td>14000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1     2     3     4     5     6     7     8     9    All\n",
       "True                                                                        \n",
       "0          1305     0     0     0     0     0     6     1     0     0   1312\n",
       "1             0  1595     2     0     0     0     1     4     0     2   1604\n",
       "2             7     8  1311     4     0     0     1    15     0     2   1348\n",
       "3             0     4     9  1377     1    18     0     6     8     4   1427\n",
       "4             1    11     0     0  1311     0     7     3     1    28   1362\n",
       "5             2     0     2    14     0  1238    13     1     5     5   1280\n",
       "6             3     1     1     0     2     3  1386     0     1     0   1397\n",
       "7             1     9     5     1     4     0     0  1428     1    12   1461\n",
       "8             1    14     1    15     5    14     6     1  1324     9   1390\n",
       "9             4     3     2     9    18     2     1    10     3  1367   1419\n",
       "All        1324  1645  1333  1420  1341  1275  1421  1469  1343  1429  14000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test, y_test_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Classification Metrics\n",
    "\n",
    "We can build a text report showing the main classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1312\n",
      "           1       0.97      0.99      0.98      1604\n",
      "           2       0.98      0.97      0.98      1348\n",
      "           3       0.97      0.96      0.97      1427\n",
      "           4       0.98      0.96      0.97      1362\n",
      "           5       0.97      0.97      0.97      1280\n",
      "           6       0.98      0.99      0.98      1397\n",
      "           7       0.97      0.98      0.97      1461\n",
      "           8       0.99      0.95      0.97      1390\n",
      "           9       0.96      0.96      0.96      1419\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Training Time Comparison with Random Forest Classifier\n",
    "\n",
    "\n",
    "We train a Random Forest classifier using the MNIST data set.\n",
    "\n",
    "We observe the following.\n",
    "- Training time is significantly faster (48 minutes for K-NN vs 2 min 15 sec for Random Forest classifier)\n",
    "- The performane of the Random Forest classifier is as good as the K-NN classifier\n",
    "\n",
    "We use optimal hypeparameters to train the Random Forest classifier.\n",
    "\n",
    "For a detail discussion on Random Forest classifier see the github repository **\"Decision Tree - Random Forest: Smart Tricks For Performance Enhancement\"**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: Test Accuracy:  0.9711428571428572\n",
      "\n",
      "Random Forest: Test Confusion Matrix:\n",
      "[[1302    0    1    0    0    1    4    0    4    0]\n",
      " [   0 1581    7    3    3    2    1    4    2    1]\n",
      " [   2    3 1310    6    6    0    4    7    9    1]\n",
      " [   1    0   13 1373    0   13    0    8   12    7]\n",
      " [   2    1    1    0 1319    0    8    3    4   24]\n",
      " [   2    0    3   10    0 1238   11    2    9    5]\n",
      " [   8    2    1    0    0    6 1377    0    3    0]\n",
      " [   1    3   19    1    8    0    0 1411    2   16]\n",
      " [   1    6    8    5    6    8    3    2 1339   12]\n",
      " [   9    3    4   23   15    4    1   10    4 1346]]\n",
      "\n",
      "Random Forest: Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1312\n",
      "           1       0.99      0.99      0.99      1604\n",
      "           2       0.96      0.97      0.97      1348\n",
      "           3       0.97      0.96      0.96      1427\n",
      "           4       0.97      0.97      0.97      1362\n",
      "           5       0.97      0.97      0.97      1280\n",
      "           6       0.98      0.99      0.98      1397\n",
      "           7       0.98      0.97      0.97      1461\n",
      "           8       0.96      0.96      0.96      1390\n",
      "           9       0.95      0.95      0.95      1419\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n",
      "\n",
      "\n",
      "Wall time: 2min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted_rf = forest_clf.predict(X_test)\n",
    "print(\"Random Forest: Test Accuracy: \", accuracy_score(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Combating the Curse Of Dimensionality in K-NN Using Dimensionaly Reduction\n",
    "\n",
    "To combat the curse of dimensionality problem with K-NN we can get rid of the irrelevant features by **projecting the features in a low-dimensional space**.\n",
    "\n",
    "The technique for reducing the dimension of data is known as dimensionality reduction. Our assumption is that the essence or core content of the data does not span along all dimensions.\n",
    "\n",
    "For a gentle introduction to various dimensionality reduction technique, see the github repository **\"Dimensionality Reduction\"**.\n",
    "\n",
    "We use the **Principle Component Analysis (PCA)** dimensionality reduction technique to project the MNIST dataset (784 features) to a lower dimensional space by retaining maximum variance. \n",
    "\n",
    "The goal is to see the improvement in K-NN \"training time\" due to this dimensionality reduction.\n",
    "\n",
    "Before we apply the PCA, we need to standardize the data. We have done this in the beginning of this notebook.\n",
    "\n",
    "\n",
    "## Apply PCA\n",
    "\n",
    "While applying PCA we can set the number of principle components by the \"n_components\" attribute. But more importantly, we can use this attribute to determine the % of variance we want to retain in the extracted features.\n",
    "\n",
    "For example, if we set it to 0.95, sklearn will choose the **minimum number of principal components** such that 95% of the variance is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Principle Components\n",
    "\n",
    "We can find how many components PCA chose after fitting the model by using the following attribute: n_components_\n",
    "\n",
    "We will see that 95% of the variance amounts to **154 principal components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Principle Components:  154\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Principle Components: \", pca.n_components_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Mapping (Transform) to both the Training Set and the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (PCA): Test Accuracy:  0.9765714285714285\n",
      "\n",
      "KNN (PCA): Test Confusion Matrix:\n",
      "[[1306    0    0    0    0    0    4    2    0    0]\n",
      " [   0 1594    3    0    0    0    1    4    0    2]\n",
      " [   6    5 1320    4    0    0    0   11    0    2]\n",
      " [   1    5    8 1381    1   15    0    5    7    4]\n",
      " [   1    7    0    0 1317    0    6    3    1   27]\n",
      " [   3    0    2   11    0 1237   13    1    7    6]\n",
      " [   3    1    1    0    2    3 1386    0    1    0]\n",
      " [   1    5    5    1    2    0    0 1434    1   12]\n",
      " [   1   12    1   13    5   12    5    1 1331    9]\n",
      " [   5    3    2    9   17    2    1   11    3 1366]]\n",
      "\n",
      "KNN (PCA): Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1312\n",
      "           1       0.98      0.99      0.99      1604\n",
      "           2       0.98      0.98      0.98      1348\n",
      "           3       0.97      0.97      0.97      1427\n",
      "           4       0.98      0.97      0.97      1362\n",
      "           5       0.97      0.97      0.97      1280\n",
      "           6       0.98      0.99      0.99      1397\n",
      "           7       0.97      0.98      0.98      1461\n",
      "           8       0.99      0.96      0.97      1390\n",
      "           9       0.96      0.96      0.96      1419\n",
      "\n",
      "    accuracy                           0.98     14000\n",
      "   macro avg       0.98      0.98      0.98     14000\n",
      "weighted avg       0.98      0.98      0.98     14000\n",
      "\n",
      "\n",
      "\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=4, p=2, weights=\"distance\")\n",
    "\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "y_test_predicted_pca_knn = knn_pca.predict(X_test_pca)\n",
    "print(\"KNN (PCA): Test Accuracy: \", accuracy_score(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\nKNN (PCA): Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\nKNN (PCA): Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 3 Observation\n",
    "\n",
    "We observe that after reducing the dimensionality of the dataset we are able to reduce the cost of distance calculation in K-NN, that results into significantly less \"training\" time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
